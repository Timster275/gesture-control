{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BY: https://github.com/AssanaliAbu/Hand-Gesture-Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "igMyGnjE9hEp"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2HDvhIu9hEr"
   },
   "source": [
    "# Specify each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "9NvZP2Zn9hEy"
   },
   "outputs": [],
   "source": [
    "# Specify data paths\n",
    "dataset = 'labels.csv'\n",
    "model_save_path = 'gesture_detection_model.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5oMH7x19hEz"
   },
   "source": [
    "# Set number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "du4kodXL9hEz"
   },
   "outputs": [],
   "source": [
    "# Change training classes if necessary\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjnL0uso9hEz"
   },
   "source": [
    "# Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "QT5ZqtEz9hE0"
   },
   "outputs": [],
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 3) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "QmoKFsp49hE0"
   },
   "outputs": [],
   "source": [
    "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "xQU7JTZ_9hE0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "xElG5FoPDQO9",
    "outputId": "2ef372ed-62e3-49c1-ad36-a5b5dc76701a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=int32), array([2000, 2006]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classes count\n",
    "counts = np.unique(y_dataset, return_counts=True)\n",
    "df = pd.DataFrame(counts)\n",
    "df.T.plot(kind=\"bar\", stacked=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxK_lETT9hE0"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "vHBmUf1t9hE1"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(63),\n",
    "    tf.keras.layers.Dropout(0.0),\n",
    "    # tf.keras.layers.Dense(32, activation='relu'),\n",
    "    # tf.keras.layers.Dropout(0.0),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.0),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypqky9tc9hE1",
    "outputId": "c42f3550-ceee-45b8-d40d-d99fd84a2616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_13 (Dropout)        (None, 63)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2048      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,610\n",
      "Trainable params: 2,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "MbMjOflQ9hE1"
   },
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False, save_best_only=True)\n",
    "# Callback for early stopping\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "c3Dac0M_9hE2"
   },
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XI0j1Iu9hE2"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "WirBl-JE9hE3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      " 1/70 [..............................] - ETA: 10s - loss: 0.9143 - accuracy: 0.4651\n",
      "Epoch 1: val_loss improved from inf to 0.33887, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.8678 - val_loss: 0.3389 - val_accuracy: 0.9990\n",
      "Epoch 2/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.3414 - accuracy: 1.0000\n",
      "Epoch 2: val_loss improved from 0.33887 to 0.08526, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9983 - val_loss: 0.0853 - val_accuracy: 1.0000\n",
      "Epoch 3/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0964 - accuracy: 1.0000\n",
      "Epoch 3: val_loss improved from 0.08526 to 0.02827, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
      "Epoch 4/40\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 4: val_loss improved from 0.02827 to 0.01458, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
      "Epoch 5/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 5: val_loss improved from 0.01458 to 0.00800, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 6/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 6: val_loss improved from 0.00800 to 0.00518, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 7/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 7: val_loss improved from 0.00518 to 0.00368, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 945us/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 8/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8: val_loss improved from 0.00368 to 0.00273, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 864us/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 9/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 9: val_loss improved from 0.00273 to 0.00210, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 879us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 10/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 10: val_loss improved from 0.00210 to 0.00169, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 875us/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 11/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 11: val_loss improved from 0.00169 to 0.00135, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 890us/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 12/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 9.4992e-04 - accuracy: 1.0000\n",
      "Epoch 12: val_loss improved from 0.00135 to 0.00112, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 901us/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "43/70 [=================>............] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000  \n",
      "Epoch 13: val_loss improved from 0.00112 to 0.00094, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.4249e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "50/70 [====================>.........] - ETA: 0s - loss: 8.8231e-04 - accuracy: 1.0000\n",
      "Epoch 14: val_loss improved from 0.00094 to 0.00081, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 8.9977e-04 - accuracy: 1.0000 - val_loss: 8.0628e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 6.4883e-04 - accuracy: 1.0000\n",
      "Epoch 15: val_loss improved from 0.00081 to 0.00070, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 873us/step - loss: 7.6857e-04 - accuracy: 1.0000 - val_loss: 6.9670e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 7.4603e-04 - accuracy: 1.0000\n",
      "Epoch 16: val_loss improved from 0.00070 to 0.00060, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 912us/step - loss: 6.6733e-04 - accuracy: 1.0000 - val_loss: 6.0376e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 3.1957e-04 - accuracy: 1.0000\n",
      "Epoch 17: val_loss improved from 0.00060 to 0.00053, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 934us/step - loss: 5.7679e-04 - accuracy: 1.0000 - val_loss: 5.2998e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.6777e-04 - accuracy: 1.0000\n",
      "Epoch 18: val_loss improved from 0.00053 to 0.00047, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 932us/step - loss: 5.0960e-04 - accuracy: 1.0000 - val_loss: 4.6750e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 5.5927e-04 - accuracy: 1.0000\n",
      "Epoch 19: val_loss improved from 0.00047 to 0.00042, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 910us/step - loss: 4.4952e-04 - accuracy: 1.0000 - val_loss: 4.1665e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 7.8332e-04 - accuracy: 1.0000\n",
      "Epoch 20: val_loss improved from 0.00042 to 0.00037, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 4.0139e-04 - accuracy: 1.0000 - val_loss: 3.7224e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "67/70 [===========================>..] - ETA: 0s - loss: 3.5704e-04 - accuracy: 1.0000\n",
      "Epoch 21: val_loss improved from 0.00037 to 0.00033, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 3.5827e-04 - accuracy: 1.0000 - val_loss: 3.3450e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 4.1150e-04 - accuracy: 1.0000\n",
      "Epoch 22: val_loss improved from 0.00033 to 0.00030, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 923us/step - loss: 3.2188e-04 - accuracy: 1.0000 - val_loss: 3.0164e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.4914e-04 - accuracy: 1.0000\n",
      "Epoch 23: val_loss improved from 0.00030 to 0.00027, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 2.9134e-04 - accuracy: 1.0000 - val_loss: 2.7331e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.6329e-04 - accuracy: 1.0000\n",
      "Epoch 24: val_loss improved from 0.00027 to 0.00025, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 968us/step - loss: 2.6558e-04 - accuracy: 1.0000 - val_loss: 2.5099e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.8511e-04 - accuracy: 1.0000\n",
      "Epoch 25: val_loss improved from 0.00025 to 0.00023, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 925us/step - loss: 2.3935e-04 - accuracy: 1.0000 - val_loss: 2.2732e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.4938e-04 - accuracy: 1.0000\n",
      "Epoch 26: val_loss improved from 0.00023 to 0.00021, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 922us/step - loss: 2.1935e-04 - accuracy: 1.0000 - val_loss: 2.0821e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 4.8622e-04 - accuracy: 1.0000\n",
      "Epoch 27: val_loss improved from 0.00021 to 0.00019, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 896us/step - loss: 2.0048e-04 - accuracy: 1.0000 - val_loss: 1.9126e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 2.0743e-04 - accuracy: 1.0000\n",
      "Epoch 28: val_loss improved from 0.00019 to 0.00018, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 913us/step - loss: 1.8311e-04 - accuracy: 1.0000 - val_loss: 1.7643e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.8423e-04 - accuracy: 1.0000\n",
      "Epoch 29: val_loss improved from 0.00018 to 0.00016, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 884us/step - loss: 1.6997e-04 - accuracy: 1.0000 - val_loss: 1.6224e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.3528e-04 - accuracy: 1.0000\n",
      "Epoch 30: val_loss improved from 0.00016 to 0.00015, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 980us/step - loss: 1.5685e-04 - accuracy: 1.0000 - val_loss: 1.4993e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 4.1902e-04 - accuracy: 1.0000\n",
      "Epoch 31: val_loss improved from 0.00015 to 0.00014, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 828us/step - loss: 1.4397e-04 - accuracy: 1.0000 - val_loss: 1.3908e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.3836e-04 - accuracy: 1.0000\n",
      "Epoch 32: val_loss improved from 0.00014 to 0.00013, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 831us/step - loss: 1.3368e-04 - accuracy: 1.0000 - val_loss: 1.2933e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 7.1812e-05 - accuracy: 1.0000\n",
      "Epoch 33: val_loss improved from 0.00013 to 0.00012, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 888us/step - loss: 1.2411e-04 - accuracy: 1.0000 - val_loss: 1.2005e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.0291e-04 - accuracy: 1.0000\n",
      "Epoch 34: val_loss improved from 0.00012 to 0.00011, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 1.1549e-04 - accuracy: 1.0000 - val_loss: 1.1172e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 6.8750e-05 - accuracy: 1.0000\n",
      "Epoch 35: val_loss improved from 0.00011 to 0.00010, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 846us/step - loss: 1.0725e-04 - accuracy: 1.0000 - val_loss: 1.0433e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.5101e-04 - accuracy: 1.0000\n",
      "Epoch 36: val_loss improved from 0.00010 to 0.00010, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 829us/step - loss: 1.0040e-04 - accuracy: 1.0000 - val_loss: 9.7527e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 6.7274e-05 - accuracy: 1.0000\n",
      "Epoch 37: val_loss improved from 0.00010 to 0.00009, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 861us/step - loss: 9.3307e-05 - accuracy: 1.0000 - val_loss: 9.1044e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 6.7654e-05 - accuracy: 1.0000\n",
      "Epoch 38: val_loss improved from 0.00009 to 0.00009, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 952us/step - loss: 8.7621e-05 - accuracy: 1.0000 - val_loss: 8.5391e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 1.1240e-04 - accuracy: 1.0000\n",
      "Epoch 39: val_loss improved from 0.00009 to 0.00008, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 927us/step - loss: 8.2018e-05 - accuracy: 1.0000 - val_loss: 8.0168e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      " 1/70 [..............................] - ETA: 0s - loss: 8.6966e-05 - accuracy: 1.0000\n",
      "Epoch 40: val_loss improved from 0.00008 to 0.00008, saving model to gesture_detection_model.hdf5\n",
      "70/70 [==============================] - 0s 818us/step - loss: 7.6620e-05 - accuracy: 1.0000 - val_loss: 7.5084e-05 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295e21370>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=40,\n",
    "    batch_size=43,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "RBkmDeUW9hE4"
   },
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxvb2Y299hE3",
    "outputId": "7015e279-0501-4f24-d1b5-90652d2de17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 488us/step - loss: 5.7846e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "# TODO Test on loaded model\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3U4yNWx9hE4"
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "AP1V6SCk9hE5",
    "outputId": "efce96b9-ca1c-44a5-ef77-58f1d5fc154c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, classification_report\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keypoint_model_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa3dd84ef0f650d9d8b867844db5a915f08cb58b5d6dab20fc53865ee0283ae4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
